# For local testing
#   `docker-compose -p rmloc -f docker-compose-local.yml up -d`

x-kc_dbconfig_env: &kcdbconfig_env
  KEYCLOAK_DATABASE_NAME: &kcdbname keycloak
  KEYCLOAK_DATABASE_HOST: postgres
  KEYCLOAK_DATABASE_USER: &kcdbuser keycloak
  KEYCLOAK_DATABASE_PASSWORD: &kcdbpass ${KEYCLOAK_DATABASE_PASSWORD:-keycloakpwd}  # pragma: allowlist secret

x-raesenmaeher_dbconfig_env: &rmdbconfig_env
  RM_DATABASE_NAME: &rmdbname raesenmaeher
  RM_DATABASE_HOST: postgres
  RM_DATABASE_USER: &rmdbuser raesenmaeher
  RM_DATABASE_PASSWORD: &rmdbpass ${RM_DATABASE_PASSWORD:-raesenmaeherpwd} # pragma: allowlist secret

x-tak_dbconfig_env: &takdbconfig_env
  POSTGRES_DB: &takdbname ${TAK_DATABASE_NAME:-tak}
  POSTGRES_ADDRESS: postgres
  POSTGRES_USER: &takdbuser ${TAK_DATABASE_USER:-tak}
  POSTGRES_PASSWORD: &takdbpass ${TAK_DATABASE_PASSWORD:-takdbpwd} # pragma: allowlist secret

x-postgres_env: &postgres_env
  KEYCLOAK_PASSWORD: *kcdbpass # pragma: allowlist secret
  POSTGRES_PASSWORD: &pgrootpwd ${POSTGRES_PASSWORD:-pg_R00t_user_password} # pragma: allowlist secret
  RAESENMAEHER_PASSWORD: *rmdbpass # pragma: allowlist secret
  TAK_PASSWORD: *takdbpass # pragma: allowlist secret


x-ldap_admin_env: &ldap_admin_env
  LDAP_ADMIN_PASSWORD: &ldapadminpass ${LDAP_ADMIN_PASSWORD:-ldapadminpwd} # pragma: allowlist secret
  LDAP_ADMIN_USERNAME: &ldapadminuser admin

x-keycloak_users_env: &keycloak_users_env
  KEYCLOAK_CREATE_ADMIN_USER: "true"
  KEYCLOAK_ADMIN_USER: &lcadminuser admin
  KEYCLOAK_MANAGEMENT_USER: damager
  KEYCLOAK_ADMIN_PASSWORD: &kcadminpass ${KEYCLOAK_ADMIN_PASSWORD:-kcadminpw} # pragma: allowlist secret
  KEYCLOAK_MANAGEMENT_PASSWORD: ${KEYCLOAK_MANAGEMENT_PASSWORD:-kcmgrpw}  # pragma: allowlist secret

x-keycloak_profile_env: &keycloak_profile_env
  # These can be expanded in keycloak-config/profile.json
  KCP_REALM: "RASENMAEHER"
  KCP_MAIN_ID: "4f88fe8c-ffa5-4ae4-97c9-3831a500d502"  # FIXME: get from env or something

x-keycloakinit_users_env: &keycloakinit_users_env
  KEYCLOAK_USER: *lcadminuser # pragma: allowlist secret
  KEYCLOAK_PASSWORD: *kcadminpass # pragma: allowlist secret

x-domains_env:
  SERVER_DOMAIN: &serverdomain ${SERVER_DOMAIN:-localmaeher.pvarki.fi}
  MTLS_DOMAIN: &mtlsdomein "mtls.${SERVER_DOMAIN:-localmaeher.pvarki.fi}"
  API_HTTPS_PORT: &apiport ${NGINX_HTTPS_PORT:-4439}
  PRODUCT_HTTPS_PORT: &productport ${NGINX_HTTPS_PRODUCT_PORT:-4625}
  PRODUCT_DOMAIN: &productdomain "fake.${SERVER_DOMAIN:-localmaeher.pvarki.fi}"
  TAK_DOMAIN: &takdomain "tak.${SERVER_DOMAIN:-localmaeher.pvarki.fi}"
  TAK_RMAPI_PORT: &takapiport ${TAK_RMAPI_PORT:-4626}
  KIBANA_DOMAIN: &kibanadomain "kibana.${SERVER_DOMAIN:-localmaeher.pvarki.fi}" # local Kibana
  DNS_RESOLVER_IP: &dnsresolver ${DNS_RESOLVER_IP:-127.0.0.11}  # Must be able to resolve docker internal names
  OCSCP_RESPONDER:  &publicocsp "https://${SERVER_DOMAIN:-localmaeher.pvarki.fi}:${NGINX_HTTPS_PORT:-4439}/ca/ocsp"  # The public URL

x-takbuilds: &takbuildinfo
  image: &takimage "pvarki/takserver:${TAK_RELEASE:-5.2-RELEASE-30}${DOCKER_TAG_EXTRA:-}"
  build:
    context: ./takserver
    dockerfile: Dockerfile
    args:
      TAK_RELEASE: ${TAK_RELEASE:-5.2-RELEASE-30}

x-nginxbuilds: &nginxbuildinfo
  image: pvarki/nginx:1.25${DOCKER_TAG_EXTRA:-}
  build:
    context: ./nginx
    dockerfile: Dockerfile
    target: production

x-cfssl_env: &cfssl_env
  INT_SHARED_CERT_FOLDER: /ca_public
  CFSSL_BIND_ADDRESS: ${CFSSL_BIND_ADDRESS:-0.0.0.0}
  CFSSL_BIND_PORT: &cfsslport ${CFSSL_BIND_PORT:-8888}
  CFSSL_OCSP_BIND_PORT: &oscpport ${CFSSL_OCSP_BIND_PORT:-8889}
  CFSSL_CA_NAME: ${CFSSL_CA_NAME:-LOCALMAEHER}
  OCSP_HOST: *serverdomain
  OCSP_PORT: *apiport
  CFSSL_PERSISTENT_FOLDER: /data/persistent
  OR_PORT: &ocsprestport 8887
  OR_UPSTREAM: &ocspresthost ocsprest
  CFSSL_CA_EXPIRY: &caexpiry ${CFSSL_CA_EXPIRY:-2016h}
  CFSSL_SIGN_CA_EXPIRY: *caexpiry
  CFSSL_SIGN_DEFAULT_EXPIRY: ${CFSSL_SIGN_DEFAULT_EXPIRY:-1008h}

x-takserver_env: &takserver_env
  TAK_SERVER_ADDRESS: *takdomain
  # The passwords here are just for takservers internal PKCS12 containers because it can't deal with PEM
  TAKSERVER_CERT_PASS: &takserver_cert_pass ${TAKSERVER_CERT_PASS:-takservercertpass} # pragma: allowlist secret
  TAKSERVER_KEYSTORE_PASS: *takserver_cert_pass
  CA_PASS: &tak_ca_pass ${TAK_CA_PASS:-takcacertpw} # pragma: allowlist secret
  KEYSTORE_PASS: *tak_ca_pass
  TAK_OCSP_UPSTREAM: &ocsphost "ocsp"
  TAK_OCSP_PORT: *oscpport

services:
  miniwerk:
    image: pvarki/miniwerk:${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./miniwerk
      dockerfile: Dockerfile
      target: production
    environment:
      MW_DOMAIN: *serverdomain
      MW_PRODUCTS: "fake,tak"
      MW_RASENMAEHER__API_PORT: *apiport
      MW_FAKE__API_PORT: *productport
      MW_TAK__API_PORT: *takapiport
      CAROOT: "/data/persistent/mkcert"
      MW_LE_EMAIL: "notusedwithmkcert@example.com"
      MW_LE_TEST: "true"
      MW_MKCERT: "true"
      MW_KEYTYPE: "rsa"
    ports:
      - "80:80" # For letsencrypt
    volumes:
      - kraftwerk_shared_fake:/pvarkishares/fake
      - kraftwerk_shared_tak:/pvarkishares/tak
      - kraftwerk_shared_rasenmaeher:/pvarkishares/rasenmaeher
      - kraftwerk_data:/data/persistent
      - le_certs:/le_certs
      - ca_public:/ca_public

  jwtcopy:
    image: pvarki/miniwerk:${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./miniwerk
      dockerfile: Dockerfile
      target: production
    volumes:
      - kraftwerk_data:/data/persistent
      - ca_public:/ca_public
      - ./tests/testjwts:/test_jwts
      - ./tests/testcas:/testcas
    command: ["/bin/bash", "-c", "cp /data/persistent/private/jwt.key /test_jwts/miniwerk.key && cp /data/persistent/private/jwt.pub /test_jwts/miniwerk.pub && cp -r /ca_public/* /testcas/"]
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      cfssl:
        condition: service_healthy

  cfssl:
    image: pvarki/cfssl:api-${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./cfssl
      dockerfile: Dockerfile
      target: api
    networks:
      - canet
    environment:
      <<: *cfssl_env
    volumes:
      - cfssl_data:/data/persistent
      - ca_public:/ca_public
    healthcheck:
      test: 'cfssl info -remote http://127.0.0.1:8888 || exit 1'
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  ocsp:
    image: pvarki/cfssl:ocsp-${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./cfssl
      dockerfile: Dockerfile
      target: ocsp
    networks:
      - ocspnet
    environment:
      <<: *cfssl_env
    volumes:
      - cfssl_data:/data/persistent
      - ca_public:/ca_public
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    depends_on:
      cfssl:
        condition: service_healthy
    restart: unless-stopped

  ocsprest:
    image: pvarki/cfssl:ocsprest-${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./cfssl
      dockerfile: Dockerfile
      target: ocsprest
    networks:
      - canet
    environment:
      <<: *cfssl_env
    volumes:
      - cfssl_data:/data/persistent
      - ca_public:/ca_public
    healthcheck:
      test: 'ocsprest healthcheck || exit 1'
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      cfssl:
        condition: service_healthy
    restart: unless-stopped


  postgres:
    platform: linux/amd64  # It seems there are no arm images available
    image: postgis/postgis:15-3.4
    networks:
      - dbnet
    ports:
      - "5432:5432"  # ONLY for testing, do not expose in production
    volumes:
      - ./pg_init:/docker-entrypoint-initdb.d
      - pg_data:/var/lib/postgresql/data
      - ca_public:/ca_public
    environment:
      <<: *postgres_env
    depends_on:
      cfssl:
        condition: service_healthy
    healthcheck:
      test: "pg_isready --dbname=$$POSTGRES_DB --username=$$POSTGRES_USER || exit 1"
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  openldap_cert_perms:  # FIXME: make a separate volume or something and copy the certs for correct user under it
    image: bash:latest
    volumes:
      - le_certs:/le_certs
    command: ["/usr/local/bin/bash", "-c", "chmod a+rwx -R /le_certs"]
    depends_on:
      miniwerk:
        condition: service_completed_successfully

  openldap:
    image: pvarki/openldap:${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./openldap
      dockerfile: Dockerfile
    networks:
      - kcnet
      - dbnet
    ports:
      - '1636:1636'  # LDAPs
    environment:
      <<: *ldap_admin_env
      LDAP_SKIP_DEFAULT_TREE: "yes"
      LDAP_ALLOW_ANON_BINDING: "no"
      # FIXME: get from env ??
      LDAP_ROOT: "dc=example,dc=org"  # Probably needs to match the custom ldfis
      LDAP_LOGLEVEL: 0
      LDAP_ENABLE_TLS: "yes"
      LDAP_TLS_CERT_FILE: /le_certs/rasenmaeher/fullchain.pem
      LDAP_TLS_KEY_FILE: /le_certs/rasenmaeher/privkey.pem
      LDAP_TLS_CA_FILE: /ca_public/ca_chain.pem
      LDAP_TLS_DH_PARAM_FILENAME: /dhparam.pem
    volumes:
      - openldap_data:/bitnami/openldap
      - ca_public:/ca_public
      - le_certs:/le_certs
      - ./nginx/includes/dhparam.pem:/dhparam.pem
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      cfssl:
        condition: service_healthy
    healthcheck:
      # This started returning: No such object (32)
      #test: 'ldapsearch -Q -tt -LLL -Y EXTERNAL -H ldapi:/// "(uid=testuser)" -b dc=example,dc=org memberOf || exit 1'
      test: "true" # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped

  keycloak:
    image: bitnami/keycloak:21.1.2
    environment:
      <<: [*kcdbconfig_env, *keycloak_users_env]
      KC_HEALTH_ENABLED: "true"
    networks:
      - kcnet
      - dbnet
    volumes:
      - ca_public:/ca
    depends_on:
      openldap:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: "curl -s localhost:8080/health/ready | grep status | grep UP || exit 1"
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: unless-stopped

  # init container that sets up profile with realm on keycloak instance
  keycloak-init:
    image: adorsys/keycloak-config-cli:latest-21.1.1
    networks:
      - kcnet
    volumes:
      - ./keycloak/keycloak-config:/config
      - ca_public:/ca
    environment:
      <<: [*keycloak_profile_env, *ldap_admin_env, *keycloakinit_users_env]
      KEYCLOAK_URL: http://keycloak:8080
      KEYCLOAK_SSL-VERIFY: "false"  # We are using the internal port
      KEYCLOAK_AVAILABILITYCHECK_ENABLED: "true"
      KEYCLOAK_AVAILABILITYCHECK_TIMEOUT: 30s
      IMPORT_VAR_SUBSTITUTION_ENABLED: "true"
      LDAP_CONNECTION_URL: ldap://openldap:1389
    depends_on:
      keycloak:
        condition: service_healthy

  rmapi:
    image: pvarki/rmapi:${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./api
      dockerfile: Dockerfile
      target: production
    ports:
      - "8000:8000"  # NOTE: Do *NOT* expose this in production, always pass through NGinx proxy
    environment:
      <<: [*rmdbconfig_env]
      JWT_PUBKEY_PATH: "/data/persistent/public"
      JWT_PRIVKEY_PATH: "/data/persistent/private/rm_jwtsign.key"
      JWT_PRIVKEY_PASS: "localmaeherjwtkeypass"  # pragma: allowlist secret
      RM_CFSSL_HOST: "http://cfssl"
      RM_CFSSL_PORT: *cfsslport
      RM_OCSPREST_HOST: "http://ocsprest"
      RM_OCSPREST_PORT: 8887
      JWT_LIFETIME: "3456000"  # Long JWT lifetimes for testing
      LOG_CONSOLE_FORMATTER: "local"
      RM_OCSCP_RESPONDER: *publicocsp
      #RM_TILAUSPALVELU_JWT: "file:///pvarki/publickeys/kraftwerk.pub"
      RM_TILAUSPALVELU_JWT: ""
    networks:
      - apinet
      - kcnet
      - canet
      - intranet
      - dbnet
      - ocspnet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ca_public:/ca_public
      - rmapi_data:/data/persistent
      - kraftwerk_shared_rasenmaeher:/pvarki
    depends_on:
      cfssl:
        condition: service_healthy
      ocsprest:
        condition: service_healthy
      keycloak:
        condition: service_healthy
      postgres:  # keycloak already depends on pg but for completenes' sake
        condition: service_healthy
      keycloak-init:
        condition: service_completed_successfully
      miniwerk:
        condition: service_completed_successfully
    healthcheck:
      test: 'rasenmaeher_api healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  rmui:
    image: pvarki/rmui:${VITE_ASSET_SET:-neutral}-${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./ui
      dockerfile: Dockerfile
      target: production
      args:
        VITE_ASSET_SET: ${VITE_ASSET_SET:-neutral}
    volumes:
      - rmui_files:/deliver

  nginx_templates:
    image: pvarki/nginx:templates-${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./nginx
      dockerfile: Dockerfile
      target: configs
    volumes:
      - nginx_templates:/deliver

  rmnginx:
    <<: *nginxbuildinfo
    volumes:
      - nginx_templates:/nginx_templates
      - ca_public:/ca_public
      - le_certs:/le_certs
      - rmui_files:/rmui_files
    environment:
      <<: *cfssl_env
      NGINX_HOST: *serverdomain
      NGINX_HTTP_PORT: ${NGINX_HTTP_PORT:-8015}
      NGINX_HTTPS_PORT: *apiport
      NGINX_UPSTREAM: "rmapi"
      NGINX_UPSTREAM_PORT: "8000"
      NGINX_UI_UPSTREAM: "rmui"
      NGINX_UI_UPSTREAM_PORT: ${NGINX_UI_UPSTREAM_PORT:-8002}
      NGINX_CERT_NAME: "rasenmaeher"
      NGINX_OCSP_UPSTREAM: *ocsphost
      DNS_RESOLVER_IP: *dnsresolver
      NGINX_TEMPLATE_DIR: "templates_rasenmaeher"
    networks:
      - apinet
      - kcnet
      - intranet
      - ocspnet
    ports:
      - "${NGINX_HTTP_PORT:-8015}:${NGINX_HTTP_PORT:-8015}"
      - "${NGINX_HTTPS_PORT:-4439}:${NGINX_HTTPS_PORT:-4439}"
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      keycloak:
        condition: service_healthy
      rmapi:
        condition: service_healthy
      rmui:
        condition: service_completed_successfully
      nginx_templates:
        condition: service_completed_successfully
      ocsp:
        condition: service_healthy
      ocsprest:
        condition: service_healthy
      cfssl:
        condition: service_healthy
    healthcheck:
      test: 'curl -s localhost:5666/healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  kwinit:  # Mostly to make sure it's built
    image: pvarki/kw_product_init:${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./kw_product_init
      dockerfile: Dockerfile
      target: production
    command: ["help"]
    volumes:
      - ca_public:/ca_public
      - le_certs:/le_certs
      - kraftwerk_shared_fake:/pvarki
      - kwinit_data:/data/persistent

############################
# BEGIN Elastic & Kibana   #
############################

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.3
    container_name: elasticsearch
    environment:
      - node.name=elasticsearch
      - discovery.type=single-node
      - network.host=0.0.0.0
      - bootstrap.memory_lock=true
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks:
      - loggingnet
    ports:
      - "9200:9200"
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:8.15.3
    container_name: kibana
    environment:
      - SERVER_NAME=kibana
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - XPACK_SECURITY_ENABLED=false
    depends_on:
      - elasticsearch
    networks:
      - loggingnet
    expose:
      - "5601"
    restart: unless-stopped

  kibananginx:
    <<: *nginxbuildinfo
    volumes:
      - nginx_templates:/nginx_templates
      - ca_public:/ca_public
      - le_certs:/le_certs
    environment:
      NGINX_HOST: "kibana.${SERVER_DOMAIN}"
      NGINX_HTTP_PORT: "80"
      NGINX_HTTPS_PORT: "5601"
      NGINX_UPSTREAM: "kibana"
      NGINX_UPSTREAM_PORT: "5601"
      NGINX_CERT_NAME: "rasenmaeher"
      CFSSL_OCSP_BIND_PORT: *oscpport
      NGINX_OCSP_UPSTREAM: *ocsphost
      DNS_RESOLVER_IP: *dnsresolver
      NGINX_TEMPLATE_DIR: "templates_kibana"
    networks:
      - loggingnet
      - ocspnet
    ports:
      - "5601:5601"
    depends_on:
      kibana:
        condition: service_started
      nginx_templates:
        condition: service_completed_successfully
      ocsp:
        condition: service_healthy
      cfssl:
        condition: service_healthy
    healthcheck:
      test: 'curl -s localhost:5666/healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  elastic-agent:
    image: docker.elastic.co/beats/elastic-agent:8.15.3
    container_name: elastic-agent
    user: root
    environment:
      ELASTIC_AGENT_ENABLED: "false"  # Disable Fleet enrollment in local mode
      ELASTIC_AGENT_ID: "local-agent"  # Assign a unique ID
      ELASTIC_AGENT_ENROLLMENT_TOKEN: "dummy-token"
      ELASTIC_AGENT_FLEET_URL: "http://dummy-fleet-url"
      ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
      LOG_INPUT_ENABLED: "true"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /:/hostfs:ro
      - ./elastic-agent-config/elastic-agent.yml:/usr/share/elastic-agent/elastic-agent.yml
    networks:
      - loggingnet
    restart: unless-stopped


##########################
# END Elastic & Kibana   #
##########################

######################
# Begin: Fakeproduct #
######################
  rmfpapi:
    image: pvarki/rmfpapi:${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}
    build:
      context: ./fpintegration
      dockerfile: Dockerfile
      target: production
    environment:
      LOG_CONSOLE_FORMATTER: "local"
    ports:
      - "8001:8001"  # NOTE: Do *NOT* expose this in production, always pass through NGinx proxy
    networks:
      - productnet
      - intranet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ca_public:/ca_public
      - kraftwerk_shared_fake:/pvarki
      - rmfpapi_data:/data/persistent
    depends_on:
      rmnginx:
        condition: service_healthy
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  fpnginx:
    <<: *nginxbuildinfo
    volumes:
      - nginx_templates:/nginx_templates
      - ca_public:/ca_public
      - le_certs:/le_certs
    environment:
      NGINX_HOST: *productdomain
      NGINX_HTTPS_PORT: *productport
      NGINX_UPSTREAM: "rmfpapi"
      NGINX_UPSTREAM_PORT: "8001"
      NGINX_CERT_NAME: "rasenmaeher"
      CFSSL_OCSP_BIND_PORT: *oscpport
      NGINX_OCSP_UPSTREAM: *ocsphost
      DNS_RESOLVER_IP: *dnsresolver
      NGINX_TEMPLATE_DIR: "templates_productapi"
    networks:
      - productnet
      - intranet
      - ocspnet
    ports:
      - "${NGINX_HTTP_PRODUCT_PORT:-8016}:${NGINX_HTTP_PRODUCT_PORT:-8016}"
      - "${NGINX_HTTPS_PRODUCT_PORT:-4625}:${NGINX_HTTPS_PRODUCT_PORT:-4625}"
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      nginx_templates:
        condition: service_completed_successfully
      rmfpapi:
        condition: service_healthy
    healthcheck:
      test: 'curl -s localhost:5666/healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped
####################
# End: Fakeproduct #
####################

##############
# Begin: TAK #
##############
  takinit:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      rmnginx:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - taknet
      - dbnet
    volumes:
      - kraftwerk_shared_tak:/pvarki
      - tak_data:/opt/tak/data
      - le_certs:/le_certs
      - ca_public:/ca_public
    command: /opt/scripts/firstrun_rm.sh

  takconfig:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      takinit:
          condition: service_completed_successfully
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
      - le_certs:/le_certs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - taknet
      - dbnet
      - intranet
      - ocspnet
    ports:
      - '8443:8443'
      - '8444:8444'
      - '8446:8446'
      - '8089:8089'
      - '8080:8080'  # NOTE: Do *NOT* expose this in production, this is non-tls port for debugging
      - "8003:8003"  # NOTE: Do *NOT* expose this in production, this is the port for takrmapi
    command: ./opt/scripts/start-tak.sh config
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takmsg:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takconfig:
        condition: service_healthy
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
      - le_certs:/le_certs
    network_mode: "service:takconfig"
    command: ./opt/scripts/start-tak.sh messaging
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takapi:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takmsg:
        condition: service_healthy
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
      - le_certs:/le_certs
    network_mode: "service:takconfig"
    command: ./opt/scripts/start-tak.sh api
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takreten:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takmsg:
        condition: service_healthy
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
    network_mode: "service:takconfig"
    command: ./opt/scripts/start-tak.sh retention
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takplug:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takapi:
        condition: service_healthy
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
    network_mode: "service:takconfig"
    command: ./opt/scripts/start-tak.sh pm
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takrmapi:
    image: pvarki/takrmapi:${PVARKI_VERSION_TAG:-latest}${DOCKER_TAG_EXTRA:-}-tak${TAK_RELEASE:-5.2-RELEASE-30}
    build:
      context: ./takintegration
      dockerfile: Dockerfile
      target: production
      args:
        TAKSERVER_IMAGE: *takimage
    network_mode: "service:takconfig"
    environment:
      LOG_CONSOLE_FORMATTER: "local"
    volumes:
      - ca_public:/ca_public
      - le_certs:/le_certs
      - kraftwerk_shared_tak:/pvarki
      - takrmapi_data:/data/persistent
      - tak_data:/opt/tak/data
    depends_on:
      rmnginx:
        condition: service_healthy
      takmsg:
        condition: service_healthy
      takapi:
        condition: service_healthy
    healthcheck:
      test: 'takrmapi healthcheck || exit 1'
      interval: 15s
      timeout: 35s
      retries: 3
      start_period: 45s
    restart: unless-stopped

  taknginx:
    <<: *nginxbuildinfo
    volumes:
      - nginx_templates:/nginx_templates
      - ca_public:/ca_public
      - le_certs:/le_certs
    environment:
      NGINX_HOST: *takdomain
      NGINX_HTTPS_PORT: *takapiport
      NGINX_UPSTREAM: "takconfig"  # Due to the sidecar thing we must use the config container as host
      NGINX_UPSTREAM_PORT: "8003"
      NGINX_CERT_NAME: "rasenmaeher"
      CFSSL_OCSP_BIND_PORT: *oscpport
      NGINX_OCSP_UPSTREAM: *ocsphost
      DNS_RESOLVER_IP: *dnsresolver
      NGINX_TEMPLATE_DIR: "templates_productapi"
    networks:
      - taknet
      - intranet
      - ocspnet
    ports:
      - "${TAK_RMAPI_PORT:-4626}:${TAK_RMAPI_PORT:-4626}"
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      nginx_templates:
        condition: service_completed_successfully
      takrmapi:
        condition: service_healthy
    healthcheck:
      test: 'curl -s localhost:5666/healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped
############
# End: TAK #
############


####################
# Shared resources #
####################
networks:
  productnet:
  apinet:
  kcnet:
  canet:
  ocspnet:
  dbnet:
  intranet:
  taknet:
  loggingnet:

volumes:
  kraftwerk_data:
  kwinit_data:
  kraftwerk_shared_fake:
  kraftwerk_shared_tak:
  kraftwerk_shared_rasenmaeher:
  pg_data:
  cfssl_data:
  openldap_data:
  rmapi_data:
  rmfpapi_data:
  ca_public:
  le_certs:
  tak_data:
  takrmapi_data:
  rmui_files:
  nginx_templates:
  es_data:
