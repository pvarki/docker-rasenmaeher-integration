# For local testing
#   `docker-compose -p rmloc -f docker-compose-local.yml up -d`

x-domains_env:
  SERVER_DOMAIN: &serverdomain ${SERVER_DOMAIN:-localmaeher.dev.pvarki.fi}
  MTLS_DOMAIN: &mtlsdomein "mtls.${SERVER_DOMAIN:-localmaeher.dev.pvarki.fi}"
  API_HTTPS_PORT: &apiport ${NGINX_HTTPS_PORT:-4439}
  PRODUCT_HTTPS_PORT: &productport ${NGINX_HTTPS_PRODUCT_PORT:-4626}
  PRODUCT_HTTPS_EPHEMERAL_PORT: &productephemeralport ${NGINX_HTTPS_EPHEMERAL_PRODUCT_PORT:-4627}
  TAK_DOMAIN: &takdomain "tak.${SERVER_DOMAIN:-localmaeher.dev.pvarki.fi}"
  KC_DOMAIN:  &kcdomain "kc.${SERVER_DOMAIN:-localmaeher.dev.pvarki.fi}"
  MTX_DOMAIN:  &mtxdomain "mtx.${SERVER_DOMAIN:-localmaeher.dev.pvarki.fi}"
  DNS_RESOLVER_IP: &dnsresolver ${DNS_RESOLVER_IP:-127.0.0.11}  # Must be able to resolve docker internal names
  OCSCP_RESPONDER:  &publicocsp "https://${SERVER_DOMAIN:-localmaeher.dev.pvarki.fi}:${NGINX_HTTPS_PORT:-4439}/ca/ocsp"  # The public URL

x-kc_dbconfig_env: &kcdbconfig_env
  KEYCLOAK_DATABASE_NAME: &kcdbname keycloak
  KEYCLOAK_DATABASE_HOST: postgres
  KEYCLOAK_DATABASE_USER: &kcdbuser keycloak
  KEYCLOAK_DATABASE_PASSWORD: &kcdbpass ${KEYCLOAK_DATABASE_PASSWORD:-keycloakpwd}  # pragma: allowlist secret

x-raesenmaeher_dbconfig_env: &rmdbconfig_env
  RM_DATABASE_NAME: &rmdbname raesenmaeher
  RM_DATABASE_HOST: postgres
  RM_DATABASE_USER: &rmdbuser raesenmaeher
  RM_DATABASE_PASSWORD: &rmdbpass ${RM_DATABASE_PASSWORD:-raesenmaeherpwd} # pragma: allowlist secret

x-ldap_admin_env: &ldap_admin_env
  LDAP_ADMIN_PASSWORD: &ldapadminpass ${LDAP_ADMIN_PASSWORD:-ldapadminpwd} # pragma: allowlist secret
  LDAP_ADMIN_USERNAME: &ldapadminuser admin

x-mtxauthz_dbconfig_env: &rmmtx_dbconfig_env
  RMMTX_DATABASE_DATABASE: &rmmtx_dbname ${RMMTX_DATABASE_DATABASE:-rmmtx}
  RMMTX_DATABASE_HOST: postgres
  RMMTX_DATABASE_USER: &rmmtx_dbuser ${RMMTX_DATABASE_USER:-rmmtx}
  RMMTX_DATABASE_PASSWORD: &rmmtx_dbpass ${RMMTX_DATABASE_PASSWORD:-mtxauthzdbpwd} # pragma: allowlist secret
  RMMTX_API_PASSWORD: &rmmtx_apipass ${RMMTX_API_PASSWORD:-rmmtxpasswd} # pragma: allowlist secret

x-tak_dbconfig_env: &takdbconfig_env
  POSTGRES_DB: &takdbname ${TAK_DATABASE_NAME:-tak}
  POSTGRES_ADDRESS: postgres
  POSTGRES_USER: &takdbuser ${TAK_DATABASE_USER:-tak}
  POSTGRES_PASSWORD: &takdbpass ${TAK_DATABASE_PASSWORD:-takdbpwd} # pragma: allowlist secret
  LDAP_BIND_USER: *ldapadminuser
  LDAP_BIND_PASSWORD: *ldapadminpass

x-battlelog_dbconfig_env: &bldbconfig_env
  BL_POSTGRES_HOST: postgres
  BL_POSTGRES_DB: &bldbname battlelog
  BL_POSTGRES_USER: &bldbuser battlelog
  BL_POSTGRES_PASSWORD: &bldbpass ${BL_DATABASE_PASSWORD:-battlelogpwd} # pragma: allowlist secret
  # Currently need to be defined separately because of the way the image is built
  DATABASE_URL: "postgresql://${BL_DATABASE_USER:-battlelog}:${BL_DATABASE_PASSWORD:-battlelogpwd}@postgres:5432/battlelog"

x-postgres_env: &postgres_env
  KEYCLOAK_PASSWORD: *kcdbpass # pragma: allowlist secret
  POSTGRES_PASSWORD: &pgrootpwd ${POSTGRES_PASSWORD:-pg_R00t_user_password} # pragma: allowlist secret
  RAESENMAEHER_PASSWORD: *rmdbpass # pragma: allowlist secret
  TAK_PASSWORD: *takdbpass # pragma: allowlist secret
  BATTLELOG_PASSWORD: *bldbpass # pragma: allowlist secret
  RMMTX_PASSWORD: *rmmtx_dbpass # pragma: allowlist secret

x-keycloak_users_env: &keycloak_users_env
  KEYCLOAK_CREATE_ADMIN_USER: true
  KEYCLOAK_ADMIN_USER: &kcadminuser admin
  # FIXME: take from env (which should be set to random)
  KEYCLOAK_ADMIN_PASSWORD: &kcadminpass kcadminpw # pragma: allowlist secret

x-keycloak_profile_env: &keycloak_profile_env
  # These can be expanded in keycloak-config/profile.json
  KCP_REALM: &kc_realm "${KCP_REALM:-RASENMAEHER}"
  KCP_MAIN_ID: "${KEYCLOAK_PROFILEROOT_UUID:-4f88fe8c-ffa5-4ae4-97c9-3831a500d502}"

x-keycloak_tls_env: &keycloak_tls_env
  KEYCLOAK_ENABLE_HTTPS: true
  KCI_SERVER_HOSTNAME: *serverdomain  # TODO: Should we use a subdomain so we can do smart proxying ??
  KEYCLOAK_HTTPS_KEY_STORE_PASSWORD: ${KEYCLOAK_HTTPS_KEY_STORE_PASSWORD:-kckspwd}  # pragma: allowlist secret
  KEYCLOAK_HTTPS_TRUST_STORE_PASSWORD: ${KEYCLOAK_HTTPS_TRUST_STORE_PASSWORD:-kctspwd}  # pragma: allowlist secret
  KEYCLOAK_HTTPS_TRUST_STORE_FILE: /bitnami/keycloak/kcserver-truststore.jks
  KEYCLOAK_HTTPS_KEY_STORE_FILE: /bitnami/keycloak/kcserver-keys.jks
  KEYCLOAK_EXTRA_ARGS: "--https-client-auth=request --hostname-debug=true --cache=local"
  #KEYCLOAK_EXTRA_ARGS: "--https-client-auth=request"
  KEYCLOAK_HOSTNAME: *kcdomain
  KEYCLOAK_HTTPS_PORT: 9443

x-keycloakinit_users_env: &keycloakinit_users_env
  KEYCLOAK_USER: *kcadminuser # pragma: allowlist secret
  KEYCLOAK_PASSWORD: *kcadminpass # pragma: allowlist secret

x-takbuilds: &takbuildinfo
  image: &takimage "${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/takserver:${TAK_RELEASE:-5.6-RELEASE-6}-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}"
  build:
    context: ./takserver
    dockerfile: Dockerfile
    args:
      TAK_RELEASE: ${TAK_RELEASE:-5.6-RELEASE-6}

x-nginxbuilds: &nginxbuildinfo
  image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/nginx:1.27-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
  build:
    context: ./nginx
    dockerfile: Dockerfile
    target: production

x-cfssl_env: &cfssl_env
  INT_SHARED_CERT_FOLDER: /ca_public
  CFSSL_BIND_ADDRESS: ${CFSSL_BIND_ADDRESS:-0.0.0.0}
  CFSSL_BIND_PORT: &cfsslport ${CFSSL_BIND_PORT:-8888}
  CFSSL_OCSP_BIND_PORT: &oscpport ${CFSSL_OCSP_BIND_PORT:-8889}
  CFSSL_CA_NAME: ${CFSSL_CA_NAME:-LOCALMAEHER}
  OCSP_HOST: *serverdomain
  OCSP_PORT: *apiport
  CFSSL_PERSISTENT_FOLDER: /data/persistent
  OR_PORT: &ocsprestport 8887
  OR_UPSTREAM: &ocspresthost ocsprest
  CFSSL_CA_EXPIRY: &caexpiry ${CFSSL_CA_EXPIRY:-2016h}
  CFSSL_SIGN_CA_EXPIRY: *caexpiry
  CFSSL_SIGN_DEFAULT_EXPIRY: ${CFSSL_SIGN_DEFAULT_EXPIRY:-1008h}

x-takserver_env: &takserver_env
  TAK_SERVER_ADDRESS: *takdomain
  # The passwords here are just for takservers internal PKCS12 containers because it can't deal with PEM
  TAKSERVER_CERT_PASS: &takserver_cert_pass ${TAKSERVER_CERT_PASS:-takservercertpass} # pragma: allowlist secret
  TAKSERVER_KEYSTORE_PASS: *takserver_cert_pass
  CA_PASS: &tak_ca_pass ${TAK_CA_PASS:-takcacertpw} # pragma: allowlist secret
  KEYSTORE_PASS: *tak_ca_pass
  TAK_OCSP_ENABLE: "true"
  WEBTAK_ENABLE: "true"

x-logging: &logging
  logging:
    options:
      max-size: 50m


services:
  miniwerk:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/miniwerk:1.4.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./miniwerk
      dockerfile: Dockerfile
      target: production
    environment:
      MW_DOMAIN: *serverdomain
      MW_PRODUCTS: "tak,kc,fake,bl,mtx"
      MW_RASENMAEHER__API_PORT: *apiport
      MW_RASENMAEHER__USER_PORT: *apiport
      MW_MTX__API_PORT: *productport
      MW_MTX__USER_PORT: 9996
      MW_FAKE__API_PORT: *productport
      MW_FAKE__USER_PORT: *productport
      MW_TAK__API_PORT: *productport
      MW_TAK__USER_PORT: 8443
      MW_BL__API_BASE: "/rmapi/"
      MW_BL__API_PORT: *productport
      MW_BL__USER_PORT: *productport
      MW_TAK__EXTRAVAL: "this should be ignored"
      CAROOT: "/data/persistent/mkcert"
      MW_LE_EMAIL: "notusedwithmkcert@example.com"
      MW_LE_TEST: "true"
      MW_MKCERT: "true"
      MW_KEYTYPE: "rsa"
    ports:
      - "80:80" # For letsencrypt
    volumes:
      - kraftwerk_data:/data/persistent
      - kraftwerk_shared_rasenmaeher:/pvarkishares/rasenmaeher
      - kraftwerk_shared_fake:/pvarkishares/fake
      - kraftwerk_shared_tak:/pvarkishares/tak
      - kraftwerk_shared_bl:/pvarkishares/bl
      - kraftwerk_shared_rmmtx:/pvarkishares/mtx
      - le_certs:/le_certs
      - ca_public:/ca_public

  jwtcopy:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/miniwerk:1.4.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./miniwerk
      dockerfile: Dockerfile
      target: production
    volumes:
      - kraftwerk_data:/data/persistent
      - ca_public:/ca_public
      - ./tests/testjwts:/test_jwts
      - ./tests/testcas:/testcas
    command: ["/bin/bash", "-c", "cp /data/persistent/private/jwt.key /test_jwts/miniwerk.key && cp /data/persistent/private/jwt.pub /test_jwts/miniwerk.pub && cp -r /ca_public/* /testcas/"]
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      cfssl:
        condition: service_healthy

  cfssl:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/cfssl:api-1.2.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./cfssl
      dockerfile: Dockerfile
      target: api
    networks:
      - canet
    environment:
      <<: *cfssl_env
    volumes:
      - cfssl_data:/data/persistent
      - ca_public:/ca_public
    healthcheck:
      test: 'cfssl info -remote http://127.0.0.1:8888 || exit 1'
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  ocsp:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/cfssl:ocsp-1.2.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./cfssl
      dockerfile: Dockerfile
      target: ocsp
    networks:
      - ocspnet
    environment:
      <<: *cfssl_env
    volumes:
      - cfssl_data:/data/persistent
      - ca_public:/ca_public
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      cfssl:
        condition: service_healthy
    restart: unless-stopped

  ocsprest:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/cfssl:ocsprest-1.0.6-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./cfssl
      dockerfile: Dockerfile
      target: ocsprest
    networks:
      - canet
    environment:
      <<: *cfssl_env
    volumes:
      - cfssl_data:/data/persistent
      - ca_public:/ca_public
    healthcheck:
      test: 'ocsprest healthcheck || exit 1'
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      cfssl:
        condition: service_healthy
    restart: unless-stopped

  postgres:
    <<: *logging
    image: ${HUB_DOCKER_REPO:-nexus.dev.pvarki.fi/}postgis/postgis:15-3.4
    platform: linux/amd64  # It seems there are no arm images available
    networks:
      - dbnet
    ports:
      - "5432:5432"  # ONLY for testing, do not expose in production
    volumes:
      - ./pg_init:/docker-entrypoint-initdb.d
      - pg_data:/var/lib/postgresql/data
      - ca_public:/ca_public
    environment:
      <<: *postgres_env
    depends_on:
      cfssl:
        condition: service_healthy
    healthcheck:
      test: "pg_isready --dbname=$$POSTGRES_DB --username=$$POSTGRES_USER || exit 1"
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  openldap_cert_perms:  # FIXME: make a separate volume or something and copy the certs for correct user under it
    <<: *logging
    image: ${HUB_DOCKER_REPO:-nexus.dev.pvarki.fi/}bash:latest
    volumes:
      - le_certs:/le_certs
    command: ["/usr/local/bin/bash", "-c", "chmod a+rwx -R /le_certs"]
    depends_on:
      miniwerk:
        condition: service_completed_successfully

  openldap:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/openldap:1.0.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./keycloak/openldap
      dockerfile: Dockerfile
    networks:
      - kcnet
      - dbnet
    ports:
      - '1636:1636'  # LDAPs
    environment:
      <<: *ldap_admin_env
      LDAP_SKIP_DEFAULT_TREE: "yes"
      LDAP_ALLOW_ANON_BINDING: "no"
      # FIXME: get from env ??
      LDAP_ROOT: "dc=example,dc=org"  # Probably needs to match the custom ldfis
      LDAP_LOGLEVEL: 0
      LDAP_ENABLE_TLS: "yes"
      LDAP_TLS_CERT_FILE: /le_certs/rasenmaeher/fullchain.pem
      LDAP_TLS_KEY_FILE: /le_certs/rasenmaeher/privkey.pem
      LDAP_TLS_CA_FILE: /ca_public/ca_chain.pem
      LDAP_TLS_DH_PARAM_FILENAME: /dhparam.pem
      #LDAP_ENABLE_ACCESSLOG: "yes"
    volumes:
      - openldap_data:/bitnami/openldap
      - ca_public:/ca_public
      - le_certs:/le_certs
      - ./nginx/includes/dhparam.pem:/dhparam.pem
    depends_on:
      openldap_cert_perms:
        condition: service_completed_successfully
      miniwerk:
        condition: service_completed_successfully
      cfssl:
        condition: service_healthy
    healthcheck:
      # error 32 is back
      #test: 'ldapsearch -Q -tt -LLL -Y EXTERNAL -H ldapi:/// "(uid=admin)" -b dc=example,dc=org memberOf  || exit 1'
      test: "true"
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  keycloak_cert_perms:  # FIXME: make a separate volume or something and copy the certs for correct user under it
    <<: *logging
    image: ${HUB_DOCKER_REPO:-nexus.dev.pvarki.fi/}bash:latest
    volumes:
      - le_certs:/le_certs
    command: ["/usr/local/bin/bash", "-c", "chmod a+rwx -R /le_certs"]
    depends_on:
      miniwerk:
        condition: service_completed_successfully

  keycloak:
    <<: *logging
    image: ${HUB_DOCKER_REPO:-nexus.dev.pvarki.fi/}bitnamilegacy/keycloak:${KEYCLOAK_VERSION:-24.0.5}
    user: root
    environment:
      <<: [*kcdbconfig_env, *keycloak_users_env, *keycloak_tls_env]
      KC_HEALTH_ENABLED: "true"
      KEYCLOAK_PRODUCTION: "true"
      KC_RUN_IN_CONTAINER: "false"  # Setting this to true will just make Java hog RAM like there is no tomorrow
      KC_LOG_LEVEL: info
      #KC_LOG_LEVEL: debug
      # REMINDER: KEYCLOAK_EXTRA_ARGS is defined in keycloak_tls_env
    networks:
      - kcnet
      - dbnet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - '8280:8080'  # do not expose this in production
      - '9443:9443'
    volumes:
      - keycloak_data:/bitnami/keycloak/
      - ca_public:/ca_public
      - le_certs:/le_certs
      - ./nginx/includes/dhparam.pem:/dhparam.pem
      - ./keycloak/init_scripts:/docker-entrypoint-initdb.d
    depends_on:
      keycloak_cert_perms:
        condition: service_completed_successfully
      openldap:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: "curl -s http://keycloak:8080/health/ready | grep status | grep UP || exit 1"
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 120s
    restart: unless-stopped

  # init container that sets up profile with realm on keycloak instance
  keycloak-init:
    <<: *logging
    image: ${HUB_DOCKER_REPO:-nexus.dev.pvarki.fi/}adorsys/keycloak-config-cli:6.1.6-${INIT_KEYCLOAK_VERSION:-24}
    networks:
      - kcnet
    volumes:
      - ./keycloak/keycloak-config:/config
      - ca_public:/ca_public
    environment:
      <<: [*keycloak_profile_env, *ldap_admin_env, *keycloakinit_users_env]
      KEYCLOAK_URL: http://keycloak:8080
      KEYCLOAK_SSL-VERIFY: "false"  # We are using the internal port
      KEYCLOAK_AVAILABILITYCHECK_ENABLED: "false"
      KEYCLOAK_AVAILABILITYCHECK_TIMEOUT: 15s
      IMPORT_VAR_SUBSTITUTION_ENABLED: "true"
      LDAP_CONNECTION_URL: ldap://openldap:1389
      LOGGING_LEVEL_KEYCLOAKCONFIGCLI: debug
      #LOGGING_LEVEL_KEYCLOAKCONFIGCLI: info
    depends_on:
      keycloak:
        condition: service_healthy

  rmapi:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/rmapi:1.12.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./api
      dockerfile: Dockerfile
      target: production
    ports:
      - "8000:8000"  # NOTE: Do *NOT* expose this in production, always pass through NGinx proxy
    environment:
      <<: [*rmdbconfig_env]
      JWT_PUBKEY_PATH: "/data/persistent/public"
      JWT_PRIVKEY_PATH: "/data/persistent/private/rm_jwtsign.key"
      JWT_PRIVKEY_PASS: "localmaeherjwtkeypass"  # pragma: allowlist secret
      EXTERNAL_JWT_PUBKEY_B64: ${RMAPI_EXTERNAL_JWT_PUBKEY_B64:-}
      RM_CFSSL_HOST: "http://cfssl"
      RM_CFSSL_PORT: *cfsslport
      RM_OCSPREST_HOST: "http://ocsprest"
      RM_OCSPREST_PORT: 8887
      JWT_LIFETIME: "3456000"  # Long JWT lifetimes for testing
      LOG_CONSOLE_FORMATTER: "local"
      RM_OCSCP_RESPONDER: *publicocsp
      #RM_TILAUSPALVELU_JWT: "file:///pvarki/publickeys/kraftwerk.pub"
      RM_TILAUSPALVELU_JWT: ""
      RM_KC_USERNAME: *kcadminuser
      RM_KC_PASSWORD: *kcadminpass  # pragma: allowlist secret
      RM_KC_REALM: *kc_realm
      UVICORN_LOG_LEVEL: "debug"
      RM_LOG_LEVEL: "DEBUG"
      RM_LOG_LEVEL_INT: "10"
      RELEASE_TAG: "local"
      RELEASE_STATUS: "alpha"
      RM_INTEGRATION_API_TIMEOUT: "3.0" # Might need to raise this with slower/older CPU
    networks:
      - apinet
      - kcnet
      - canet
      - intranet
      - dbnet
      - ocspnet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ca_public:/ca_public
      - rmapi_data:/data/persistent
      - kraftwerk_shared_rasenmaeher:/pvarki
    depends_on:
      cfssl:
        condition: service_healthy
      ocsprest:
        condition: service_healthy
      keycloak:
        condition: service_healthy
      postgres:  # keycloak already depends on pg but for completenes' sake
        condition: service_healthy
      keycloak-init:
        condition: service_completed_successfully
      miniwerk:
        condition: service_completed_successfully
    healthcheck:
      test: 'rasenmaeher_api healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # FIXME: New UI does not use the VITE_THEME ENV variable, figure out the correct:tm: way to deliver desired asset sets
  rmui:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/rmui:2.0.0-${VITE_THEME:-default}-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./uiv2
      dockerfile: Dockerfile
      target: production
      args:
        VITE_THEME: ${VITE_THEME:-default}
        RELEASE_TAG: "local"
    volumes:
      - rmui_files:/deliver

  nginx_templates:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/nginx:templates-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./nginx
      dockerfile: Dockerfile
      target: configs
    volumes:
      - nginx_templates:/deliver

  rmnginx:
    <<: [*nginxbuildinfo, *logging]
    volumes:
      - nginx_templates:/nginx_templates
      - ca_public:/ca_public
      - le_certs:/le_certs
      - rmui_files:/rmui_files
      - ui_files:/ui_files
    environment:
      <<: *cfssl_env
      NGINX_HOST: *serverdomain
      NGINX_HTTP_PORT: ${NGINX_HTTP_PORT:-8015}
      NGINX_HTTPS_PORT: *apiport
      NGINX_PRODUCTAPI_PORT: *productport
      NGINX_UPSTREAM: "rmapi"
      NGINX_UPSTREAM_PORT: "8000"
      NGINX_UI_UPSTREAM: "rmui"
      NGINX_UI_UPSTREAM_PORT: ${NGINX_UI_UPSTREAM_PORT:-8002}
      NGINX_CERT_NAME: "rasenmaeher"
      NGINX_OCSP_UPSTREAM: "ocsp"
      DNS_RESOLVER_IP: *dnsresolver
      NGINX_TEMPLATE_DIR: "templates_rasenmaeher"
    networks:
      - apinet
      - kcnet
      - intranet
      - ocspnet
    ports:
      - "${NGINX_HTTP_PORT:-8015}:${NGINX_HTTP_PORT:-8015}"
      - "${NGINX_HTTPS_PORT:-4439}:${NGINX_HTTPS_PORT:-4439}"
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      keycloak:
        condition: service_healthy
      rmapi:
        condition: service_healthy
      rmui:
        condition: service_completed_successfully
      nginx_templates:
        condition: service_completed_successfully
      ocsp:
        condition: service_healthy
      ocsprest:
        condition: service_healthy
      cfssl:
        condition: service_healthy
    healthcheck:
      test: 'curl -s localhost:5666/healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped

  kwinit:  # Mostly to make sure it's built
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/kw_product_init:1.0.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./kw_product_init
      dockerfile: Dockerfile
      target: production
    command: ["help"]
    volumes:
      - ca_public:/ca_public
      - le_certs:/le_certs
      - kraftwerk_shared_fake:/pvarki
      - kwinit_data:/data/persistent

######################
# Begin: Fakeproduct #
######################
  rmfpapi:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/rmfpapi:1.4.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./fpintegration
      dockerfile: Dockerfile
      target: production
    environment:
      LOG_CONSOLE_FORMATTER: "local"
    ports:
      - "8001:8001"  # NOTE: Do *NOT* expose this in production, always pass through NGinx proxy
    networks:
      - productnet
      - intranet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ca_public:/ca_public
      - kraftwerk_shared_fake:/pvarki
      - rmfpapi_data:/data/persistent
      - ui_files:/ui_files
    depends_on:
      rmnginx:
        condition: service_healthy
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped
####################
# End: Fakeproduct #
####################

######################
# Begin: Battlelog #
######################
  blapi:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/blapi:1.0.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./battlelog
      dockerfile: Dockerfile
      target: production
    environment:
      <<: [*bldbconfig_env]
      LOG_CONSOLE_FORMATTER: "local"
    ports:
      - "8666:3000"  # NOTE: Do *NOT* expose this in production, always pass through NGinx proxy
    networks:
      - productnet
      - intranet
      - dbnet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ca_public:/ca_public
      - kraftwerk_shared_bl:/pvarki
      - blapi_data:/data/persistent
    depends_on:
      rmnginx:
        condition: service_healthy
      rmapi:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: 'curl http://localhost:3000/api/v1/healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 25s
    restart: unless-stopped
####################
# End: Battlelog #
####################

###################
# Begin: MediaMTX #
###################
  rmmtx:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/rmmtx:1.2.0-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./mtxauthz
      dockerfile: Dockerfile
      target: production
    environment:
      <<: [*rmmtx_dbconfig_env]
      RMMTX_API_URL: https://mtx.${SERVER_DOMAIN:-localmaeher.dev.pvarki.fi}:9997
      RMMTX_MTX_ADDRESS: mtx.${SERVER_DOMAIN:-localmaeher.dev.pvarki.fi}
      LOG_CONSOLE_FORMATTER: "local"
      RMMTX_LOG_LEVEL: "debug"
      UVICORN_LOG_LEVEL: "debug"
    ports:
      - "8005:8005"  # NOTE: Do *NOT* expose this in production, always pass through NGinx proxy
    networks:
      - productnet
      - intranet
      - dbnet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ca_public:/ca_public
      - kraftwerk_shared_rmmtx:/pvarki
      - rmmtx_data:/data/persistent
      - ui_files:/ui_files
    depends_on:
      rmnginx:
        condition: service_healthy
      rmapi:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: 'rmmtxauthz healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  mediamtx_cert_perms:  # FIXME: make a separate volume or something and copy the certs for correct user under it
    <<: *logging
    image: ${HUB_DOCKER_REPO:-nexus.dev.pvarki.fi/}bash:latest
    volumes:
      - le_certs:/le_certs
    command: ["/usr/local/bin/bash", "-c", "chmod a+rwx -R /le_certs"]
    depends_on:
      miniwerk:
        condition: service_completed_successfully

  mediamtx:
    <<: *logging
    image: ${HUB_DOCKER_REPO:-nexus.dev.pvarki.fi/}bluenviron/mediamtx:1.12.3
    networks:
      - productnet
      - intranet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ca_public:/ca_public
      - le_certs:/le_certs
      - ./mediamtx.yml:/mediamtx.yml
    environment:
      MTX_RTSPTRANSPORTS: "tcp"  # Can't do UDP without host mode network
      MTX_WEBRTCADDITIONALHOSTS: *serverdomain
    ports:
      - "1936:1936"
      - "8322:8322"
      - "8890:8890"
      - "9000:9000"
      - "9001:9001"
      - "9888:9888"
      - "9889:9889"
      - "9996:9996"
      - "9997:9997"  # Control API
      - "9998:9998"  # metrics: Do not expose in production
      - "9999:9999"  # pprof: Do not expose in production
      - "8890:8890/udp"
      - "8189:8189/udp"
    depends_on:
      rmmtx:
        condition: service_healthy
      mediamtx_cert_perms:
        condition: service_completed_successfully
    restart: unless-stopped
    # Can't do healthchecks because the container has nothing but the mediamtx binary
#################
# End: MediaMTX #
#################

##############
# Begin: TAK #
##############
  takinit:
    <<: [*takbuildinfo, *logging]
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      rmnginx:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - taknet
      - dbnet
    volumes:
      - ./takserver/update:/opt/tak/webcontent/update
      - kraftwerk_shared_tak:/pvarki
      - tak_data:/opt/tak/data
      - le_certs:/le_certs
      - ca_public:/ca_public
    command: /opt/scripts/firstrun_rm.sh

  takconfig:
    <<: [*takbuildinfo, *logging]
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      takinit:
          condition: service_completed_successfully
    volumes:
      - ./takserver/update:/opt/tak/webcontent/update
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
      - le_certs:/le_certs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - taknet
      - dbnet
      - intranet
      - productnet
      - ocspnet
    ports:
      - '8443:8443'
      - '8444:8444'
      - '8446:8446'
      - '8089:8089'
      - '8090-8094:8090-8094' # Reserved for data feeds
      - '8080:8080'  # NOTE: Do *NOT* expose this in production, this is non-tls port for debugging
      - "8003:8003"  # NOTE: Do *NOT* expose this in production, this is the port for takrmapi
    command: ./opt/scripts/start-tak.sh config
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  takmsg:
    <<: [*takbuildinfo, *logging]
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takconfig:
        condition: service_healthy
    volumes:
      - ./takserver/update:/opt/tak/webcontent/update
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
      - le_certs:/le_certs
    network_mode: "service:takconfig"
    command: ./opt/scripts/start-tak.sh messaging
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  takapi:
    <<: [*takbuildinfo, *logging]
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takmsg:
        condition: service_healthy
    volumes:
      - ./takserver/update:/opt/tak/webcontent/update
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
      - le_certs:/le_certs
    network_mode: "service:takconfig"
    command: ./opt/scripts/start-tak.sh api
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  takreten:
    <<: [*takbuildinfo, *logging]
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takmsg:
        condition: service_healthy
    volumes:
      - ./takserver/update:/opt/tak/webcontent/update
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
    network_mode: "service:takconfig"
    command: ./opt/scripts/start-tak.sh retention
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  takplug:
    <<: [*takbuildinfo, *logging]
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takapi:
        condition: service_healthy
    volumes:
      - ./takserver/update:/opt/tak/webcontent/update
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
    network_mode: "service:takconfig"
    command: ./opt/scripts/start-tak.sh pm
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  takrmapi:
    <<: *logging
    image: ${PVARKI_DOCKER_REPO:-group.docker.nexus.dev.pvarki.fi/}pvarki/takrmapi:1.7.1-tak${TAK_RELEASE:-5.6-RELEASE-6}-d${RELEASE_TAG:-2.3.1}-local${DOCKER_TAG_EXTRA:-}
    build:
      context: ./takintegration
      dockerfile: Dockerfile
      target: production
      args:
        TAKSERVER_IMAGE: *takimage
    network_mode: "service:takconfig"
    environment:
      LOG_CONSOLE_FORMATTER: "local"
      UVICORN_LOG_LEVEL: "debug"
      TI_LOG_LEVEL: "10"
      TI_MTX_SERVER_FQDN: *mtxdomain
      TI_PRODUCT_HTTPS_PORT: *productport
    volumes:
      - ca_public:/ca_public
      - le_certs:/le_certs
      - kraftwerk_shared_tak:/pvarki
      - takrmapi_data:/data/persistent
      - tak_data:/opt/tak/data
      - ui_files:/ui_files
    depends_on:
      rmnginx:
        condition: service_healthy
      takmsg:
        condition: service_healthy
      takapi:
        condition: service_healthy
    healthcheck:
      test: 'takrmapi healthcheck || exit 1'
      interval: 15s
      timeout: 35s
      retries: 3
      start_period: 80s
    restart: unless-stopped
############
# End: TAK #
############

#############################
# BEGIN: consolidated NGinx #
#############################
  productsnginx:
    <<: [*nginxbuildinfo, *logging]
    volumes:
      - nginx_templates:/nginx_templates
      - ca_public:/ca_public
      - le_certs:/le_certs
      - ui_files:/ui_files
    environment:
      NGINX_HOST: *serverdomain
      NGINX_HTTPS_PORT: *productport
      NGINX_EPHEMERAL_HTTPS_PORT: *productephemeralport
      NGINX_FP_UPSTREAM: "rmfpapi"
      NGINX_FP_UPSTREAM_PORT: "8001"
      NGINX_BL_UPSTREAM: "blapi"
      NGINX_BL_UPSTREAM_PORT: "3000"
      NGINX_TAK_UPSTREAM: "takconfig"  # Due to the sidecar thing we must use the config container as host
      NGINX_TAK_UPSTREAM_PORT: "8003"
      NGINX_RMMTX_UPSTREAM: "rmmtx"
      NGINX_RMMTX_UPSTREAM_PORT: "8005"
      NGINX_CERT_NAME: "rasenmaeher"
      NGINX_TEMPLATE_DIR: "templates_consolidated"
      CFSSL_OCSP_BIND_PORT: *oscpport
      NGINX_OCSP_UPSTREAM: "ocsp"
      DNS_RESOLVER_IP: *dnsresolver
    networks:
      - productnet
      - intranet
      - ocspnet
    ports:
      - "${NGINX_HTTPS_PRODUCT_PORT:-4626}:${NGINX_HTTPS_PRODUCT_PORT:-4626}"
      - "${NGINX_HTTPS_EPHEMERAL_PRODUCT_PORT:-4627}:${NGINX_HTTPS_EPHEMERAL_PRODUCT_PORT:-4627}"
    depends_on:
      nginx_templates:
        condition: service_completed_successfully
      rmfpapi:
        condition: service_started
      blapi:
        condition: service_started
      takrmapi:
        condition: service_started
      rmmtx:
        condition: service_started
    healthcheck:
      test: 'curl -s localhost:5666/healthcheck || exit 1'
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 25s
    restart: unless-stopped
###########################
# END: consolidated NGinx #
###########################

####################
# Shared resources #
####################
networks:
  productnet:
  apinet:
  kcnet:
  canet:
  ocspnet:
  dbnet:
  intranet:
  taknet:

volumes:
  kraftwerk_data:
  kwinit_data:
  kraftwerk_shared_fake:
  kraftwerk_shared_tak:
  kraftwerk_shared_rasenmaeher:
  pg_data:
  cfssl_data:
  openldap_data:
  keycloak_data:
  rmapi_data:
  rmfpapi_data:
  ca_public:
  le_certs:
  tak_data:
  takrmapi_data:
  rmui_files:
  nginx_templates:
  kraftwerk_shared_bl:
  blapi_data:
  kraftwerk_shared_rmmtx:
  rmmtx_data:
  ui_files:
