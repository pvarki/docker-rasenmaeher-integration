version: '3.4'

x-kc_dbconfig_env: &kcdbconfig_env
  KEYCLOAK_DATABASE_NAME: &kcdbname ${KEYCLOAK_DATABASE_NAME:-keycloak}
  KEYCLOAK_DATABASE_HOST: postgres
  KEYCLOAK_DATABASE_USER: &kcdbuser ${KEYCLOAK_DATABASE_USER:-keycloak}
  KEYCLOAK_DATABASE_PASSWORD: &kcdbpass ${KEYCLOAK_DATABASE_PASSWORD:?must be defined} # pragma: allowlist secret

x-raesenmaeher_dbconfig_env: &rmdbconfig_env
  RM_DATABASE_NAME: &rmdbname ${RM_DATABASE_NAME:-raesenmaeher}
  RM_DATABASE_HOST: postgres
  RM_DATABASE_USER: &rmdbuser ${RM_DATABASE_USER:-raesenmaeher}
  RM_DATABASE_PASSWORD: &rmdbpass ${RM_DATABASE_PASSWORD:?must be defined} # pragma: allowlist secret

x-tak_dbconfig_env: &takdbconfig_env
  POSTGRES_DB: &takdbname ${TAK_DATABASE_NAME:-tak}
  POSTGRES_ADDRESS: postgres
  POSTGRES_USER: &takdbuser ${TAK_DATABASE_USER:-tak}
  POSTGRES_PASSWORD: &takdbpass ${TAK_DATABASE_PASSWORD:?must be defined} # pragma: allowlist secret

x-postgres_env: &postgres_env
  KEYCLOAK_PASSWORD: *kcdbpass # pragma: allowlist secret
  POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?must be defined} # pragma: allowlist secret
  RAESENMAEHER_PASSWORD: *rmdbpass # pragma: allowlist secret
  TAK_PASSWORD: *takdbpass # pragma: allowlist secret


x-ldap_admin_env: &ldap_admin_env
  LDAP_ADMIN_PASSWORD: &ldapadminpass ${LDAP_ADMIN_PASSWORD:?must be defined} # pragma: allowlist secret
  LDAP_ADMIN_USERNAME: &ldapadminuser ${LDAP_ADMIN_USERNAME:-admin}

x-keycloak_users_env: &keycloak_users_env
  KEYCLOAK_CREATE_ADMIN_USER: "true"
  KEYCLOAK_ADMIN_USER: &lcadminuser admin
  KEYCLOAK_MANAGEMENT_USER: damager
  KEYCLOAK_ADMIN_PASSWORD: &kcadminpass ${KEYCLOAK_ADMIN_PASSWORD:?must be defined} # pragma: allowlist secret
  KEYCLOAK_MANAGEMENT_PASSWORD: ${KEYCLOAK_MANAGEMENT_PASSWORD:?must be defined} # pragma: allowlist secret

x-keycloak_profile_env: &keycloak_profile_env
  # These can be expanded in keycloak-config/profile.json
  KCP_REALM: "RASENMAEHER"
  KCP_MAIN_ID: "4f88fe8c-ffa5-4ae4-97c9-3831a500d502"  # FIXME: get from env or something

x-keycloakinit_users_env: &keycloakinit_users_env
  KEYCLOAK_USER: *lcadminuser # pragma: allowlist secret
  KEYCLOAK_PASSWORD: *kcadminpass # pragma: allowlist secret

x-domains_env:
  SERVER_DOMAIN: &serverdomain ${SERVER_DOMAIN:?domain must be defined}
  MTLS_DOMAIN: &mtlsdomein "mtls.${SERVER_DOMAIN:?domain must be defined}"
  API_HTTPS_PORT: &apiport ${NGINX_HTTPS_PORT:-443}
  PRODUCT_HTTPS_PORT: &productport ${NGINX_HTTPS_PRODUCT_PORT:-4625}
  PRODUCT_DOMAIN: &productdomain "fake.${SERVER_DOMAIN:?domain must be defined}"
  TAK_DOMAIN: &takdomain "tak.${SERVER_DOMAIN:?domain must be defined}"
  TAK_RMAPI_PORT: &takapiport ${TAK_RMAPI_PORT:-4626}

x-takbuilds: &takbuildinfo
  image: &takimage "pvarki/takserver:${TAK_RELEASE:-4.10-RELEASE-12}${DOCKER_TAG_EXTRA:-}"
  build:
    context: ./takserver
    dockerfile: Dockerfile

x-takserver_env: &takserver_env
  TAK_SERVER_ADDRESS: *takdomain
  # The passwords here are just for takservers internal PKCS12 containers because it can't deal with PEM
  TAKSERVER_CERT_PASS: &takserver_cert_pass ${TAKSERVER_CERT_PASS:?must be defined} # pragma: allowlist secret
  TAKSERVER_KEYSTORE_PASS: *takserver_cert_pass
  CA_PASS: &tak_ca_pass ${TAK_CA_PASS:?must be defined} # pragma: allowlist secret
  KEYSTORE_PASS: *tak_ca_pass


x-cfssl_env: &cfssl_env
  INT_SHARED_CERT_FOLDER: /ca_public
  CFSSL_BIND_ADDRESS: ${CFSSL_BIND_ADDRESS:-0.0.0.0}
  CFSSL_BIND_PORT: &cfsslport ${CFSSL_BIND_PORT:-8888}
  CFSSL_OCSP_BIND_PORT: &oscpport ${CFSSL_OCSP_BIND_PORT:-8889}
  CFSSL_CA_NAME: ${CFSSL_CA_NAME:?ca name must be defined}
  OCSP_HOST: *serverdomain
  OCSP_PORT: *apiport
  CFSSL_PERSISTENT_FOLDER: /data/persistent

services:
  miniwerk:
    image: pvarki/miniwerk:latest
    build:
      context: ./miniwerk
      dockerfile: Dockerfile
      target: production
    environment:
      MW_DOMAIN: *serverdomain
      MW_RASENMAEHER__API_PORT: *apiport
      MW_FAKE__API_PORT: *productport
      MW_TAK__API_PORT: *takapiport
      MW_LE_EMAIL: ${MW_LE_EMAIL?LE contact email must be defined}
      MW_LE_TEST: ${MW_LE_TEST:-true}  # see example_env.sh
      MW_KEYTYPE: "rsa"
    volumes:
      - kraftwerk_shared_fake:/pvarkishares/fake
      - kraftwerk_shared_tak:/pvarkishares/tak
      - kraftwerk_shared_rasenmaeher:/pvarkishares/rasenmaeher
      - kraftwerk_data:/data/persistent
      - ./le_state:/data/persistent/le/conf  # At least during testing we want to keep LE certs outside of the volumes
      - le_certs:/le_certs
    ports:
      - "80:80"

  cfssl:
    image: pvarki/cfssl:api-latest
    build:
      context: ./cfssl
      dockerfile: Dockerfile
      target: api
    networks:
      - canet
    environment:
      <<: *cfssl_env
    volumes:
      - cfssl_data:/data/persistent
      - ca_public:/ca_public
    healthcheck:
      test: 'cfssl info -remote http://127.0.0.1:8888'
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  ocsp:
    image: pvarki/cfssl:ocsp-latest
    build:
      context: ./cfssl
      dockerfile: Dockerfile
      target: ocsp
    networks:
      - ocspnet
    environment:
      <<: *cfssl_env
    volumes:
      - cfssl_data:/data/persistent
      - ca_public:/ca_public
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    depends_on:
      cfssl:
        condition: service_healthy
    restart: unless-stopped

  postgres:
    image: postgis/postgis:15-3.4
    networks:
      - dbnet
    volumes:
      - ./pg_init:/docker-entrypoint-initdb.d
      - pg_data:/var/lib/postgresql/data
      - ca_public:/ca_public
    environment:
      <<: *postgres_env
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      cfssl:
        condition: service_healthy
    healthcheck:
      test: "pg_isready --dbname=$$POSTGRES_DB --username=$$POSTGRES_USER"
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  openldap_cert_perms:  # FIXME: make a separate volume or something and copy the certs for correct user under it
    image: bash:latest
    volumes:
      - le_certs:/le_certs
    command: ["/usr/local/bin/bash", "-c", "chmod a+rwx -R /le_certs"]
    depends_on:
      miniwerk:
        condition: service_completed_successfully

  openldap:
    image: pvarki/openldap:latest
    build:
      context: ./openldap
      dockerfile: Dockerfile
    networks:
      - kcnet
      - dbnet
    ports:
      - '1636:1636'  # LDAPs
    environment:
      <<: *ldap_admin_env
      LDAP_SKIP_DEFAULT_TREE: "yes"
      LDAP_ALLOW_ANON_BINDING: "no"
      # FIXME: get from env ??
      LDAP_ROOT: "dc=example,dc=org"  # Probably needs to match the custom ldfis
      LDAP_LOGLEVEL: 0
      LDAP_ENABLE_TLS: "yes"
      LDAP_TLS_CERT_FILE: /le_certs/rasenmaeher/fullchain.pem
      LDAP_TLS_KEY_FILE: /le_certs/rasenmaeher/privkey.pem
      LDAP_TLS_CA_FILE: /ca_public/ca_chain.pem
      LDAP_TLS_DH_PARAM_FILENAME: /dhparam.pem
    volumes:
      - openldap_data:/bitnami/openldap
      - ca_public:/ca_public
      - le_certs:/le_certs
      - ./nginx/includes/dhparam.pem:/dhparam.pem
    depends_on:
      openldap_cert_perms:
        condition: service_completed_successfully
      miniwerk:
        condition: service_completed_successfully
      cfssl:
        condition: service_healthy
    healthcheck:
      # This started returning: No such object (32)
      #test: 'ldapsearch -Q -tt -LLL -Y EXTERNAL -H ldapi:/// "(uid=testuser)" -b dc=example,dc=org memberOf'
      test: "true" # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped

  keycloak:
    image: bitnami/keycloak:21.0.2
    environment:
      <<: [*kcdbconfig_env, *keycloak_users_env]
      KC_HEALTH_ENABLED: "true"
    networks:
      - kcnet
      - dbnet
    volumes:
      - ca_public:/ca
    depends_on:
      openldap:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: "curl -s localhost:8080/health/ready | grep status | grep UP"
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 15s
    restart: unless-stopped

  # init container that sets up profile with realm on keycloak instance
  keycloak-init:
    image: adorsys/keycloak-config-cli:latest-21.0.1
    networks:
      - kcnet
    volumes:
      - ./keycloak/keycloak-config:/config
      - ca_public:/ca
    environment:
      <<: [*keycloak_profile_env, *ldap_admin_env, *keycloakinit_users_env]
      KEYCLOAK_URL: http://keycloak:8080
      KEYCLOAK_SSL-VERIFY: "false"  # We are using the internal port
      KEYCLOAK_AVAILABILITYCHECK_ENABLED: "true"
      KEYCLOAK_AVAILABILITYCHECK_TIMEOUT: 30s
      IMPORT_VAR_SUBSTITUTION_ENABLED: "true"
      LDAP_CONNECTION_URL: ldap://openldap:1389
    depends_on:
      keycloak:
        condition: service_healthy

  rmapi:
    image: pvarki/rmapi:latest
    build:
      context: ./api
      dockerfile: Dockerfile
      target: production
    environment:
      <<: [*rmdbconfig_env]
      JWT_PUBKEY_PATH: "/data/persistent/public"
      JWT_PRIVKEY_PATH: "/data/persistent/private/rm_jwtsign.key"
      RM_CFSSL_HOST: "http://cfssl"
      RM_CFSSL_PORT: *cfsslport
      JWT_LIFETIME: "3456000"  # Long JWT lifetimes for testing
    networks:
      - apinet
      - kcnet
      - canet
      - intranet
      - dbnet
    volumes:
      - ca_public:/ca_public
      - rmapi_data:/data/persistent
      - kraftwerk_shared_rasenmaeher:/pvarki
    depends_on:
      cfssl:
        condition: service_healthy
      keycloak:
        condition: service_healthy
      postgres:  # keycloak already depends on pg but for completenes' sake
        condition: service_healthy
      keycloak-init:
        condition: service_completed_successfully
      miniwerk:
        condition: service_completed_successfully
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  rmui:
    image: pvarki/rmui:latest
    build:
      context: ./ui
      dockerfile: Dockerfile
      target: production
    volumes:
      - rmui_files:/deliver

  rmnginx:
    image: nginx
    volumes:
      - ./nginx/templates_rasenmaeher:/etc/nginx/templates
      - ./nginx/includes:/etc/nginx/includes
      - ca_public:/ca_public
      - le_certs:/le_certs
      - rmui_files:/rmui_files
    environment:
      <<: *cfssl_env
      NGINX_HOST: *serverdomain
      NGINX_HTTP_PORT: ${NGINX_HTTP_PORT:-80}
      NGINX_HTTPS_PORT: *apiport
      NGINX_UPSTREAM: "rmapi"
      NGINX_UPSTREAM_PORT: "8000"
      NGINX_CRL_UPSTREAM: "cfssl"
      NGINX_OCSP_UPSTREAM: "ocsp"
      NGINX_CERT_NAME: "rasenmaeher"
      NGINX_UI_UPSTREAM: "rmui"
      NGINX_UI_UPSTREAM_PORT: ${NGINX_UI_UPSTREAM_PORT:-8002}
    networks:
      - apinet
      - kcnet
      - intranet
      - ocspnet
    ports:
      - "${NGINX_HTTPS_PORT:-443}:${NGINX_HTTPS_PORT:-443}"
    depends_on:
      keycloak:
        condition: service_healthy
      rmapi:
        condition: service_healthy
      rmui:
        condition: service_completed_successfully
      ocsp:
        condition: service_healthy
      cfssl:
        condition: service_healthy
      miniwerk:
        condition: service_completed_successfully
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  kwinit:  # Mostly to make sure it's built
    image: pvarki/kw_product_init:latest
    build:
      context: ./kw_product_init
      dockerfile: Dockerfile
      target: production
    command: ["help"]
    volumes:
      - ca_public:/ca_public
      - le_certs:/le_certs
      - kraftwerk_shared_fake:/pvarki
      - kwinit_data:/data/persistent

######################
# Begin: Fakeproduct #
######################
  rmfpapi:
    image: pvarki/rmfpapi:latest
    build:
      context: ./fpintegration
      dockerfile: Dockerfile
      target: production
    networks:
      - productnet
      - intranet
    volumes:
      - ca_public:/ca_public
      - kraftwerk_shared_fake:/pvarki
      - rmfpapi_data:/data/persistent
    depends_on:
      rmnginx:
        condition: service_healthy
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  fpnginx:
    image: nginx
    volumes:
      - ./nginx/templates_productapi:/etc/nginx/templates
      - ./nginx/includes:/etc/nginx/includes
      - ca_public:/ca_public
      - le_certs:/le_certs
    environment:
      NGINX_HOST: *productdomain
      NGINX_HTTP_PORT: ${NGINX_HTTP_PRODUCT_PORT:-8080}
      NGINX_HTTPS_PORT: *productport
      NGINX_UPSTREAM: "rmfpapi"
      NGINX_UPSTREAM_PORT: "8001"
      NGINX_CERT_NAME: "rasenmaeher"
    networks:
      - productnet
      - intranet
    ports:
      - "${NGINX_HTTP_PRODUCT_PORT:-8016}:${NGINX_HTTP_PRODUCT_PORT:-8016}"
      - "${NGINX_HTTPS_PRODUCT_PORT:-4625}:${NGINX_HTTPS_PRODUCT_PORT:-4625}"
    depends_on:
      rmfpapi:
        condition: service_healthy
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped
####################
# End: Fakeproduct #
####################

##############
# Begin: TAK #
##############
  takinit:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      rmnginx:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - taknet
      - dbnet
    volumes:
      - kraftwerk_shared_tak:/pvarki
      - tak_data:/opt/tak/data
      - le_certs:/le_certs
      - ca_public:/ca_public
    command: /opt/scripts/firstrun_rm.sh

  takmsg:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      takinit:
          condition: service_completed_successfully
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
      - le_certs:/le_certs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - taknet
      - dbnet
      - intranet
    ports:
      - '8443:8443'
      - '8444:8444'
      - '8446:8446'
      - '8089:8089'
    command: ./opt/scripts/start-tak.sh messaging
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takapi:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takmsg:
        condition: service_healthy
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
      - le_certs:/le_certs
    network_mode: "service:takmsg"
    command: ./opt/scripts/start-tak.sh api
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takreten:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takmsg:
        condition: service_healthy
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
    network_mode: "service:takmsg"
    command: ./opt/scripts/start-tak.sh retention
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takplug:
    <<: *takbuildinfo
    environment:
      <<: [*takdbconfig_env, *takserver_env]
    depends_on:
      postgres:
        condition: service_healthy
      takapi:
        condition: service_healthy
    volumes:
      - tak_data:/opt/tak/data
      - ca_public:/ca_public
    network_mode: "service:takmsg"
    command: ./opt/scripts/start-tak.sh pm
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  takrmapi:
    image: pvarki/takrmapi:latest
    build:
      context: ./takintegration
      dockerfile: Dockerfile
      target: production
    network_mode: "service:takmsg"
    volumes:
      - ca_public:/ca_public
      - le_certs:/le_certs
      - kraftwerk_shared_tak:/pvarki
      - tak_data:/opt/tak/data
      - takrmapi_data:/data/persistent
    depends_on:
      rmnginx:
        condition: service_healthy
      takmsg:
        condition: service_healthy
      takapi:
        condition: service_healthy
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  taknginx:
    image: nginx
    volumes:
      - ./nginx/templates_productapi:/etc/nginx/templates
      - ./nginx/includes:/etc/nginx/includes
      - ca_public:/ca_public
      - le_certs:/le_certs
    environment:
      NGINX_HOST: *takdomain
      NGINX_HTTPS_PORT: *takapiport
      NGINX_UPSTREAM: "takmsg"  # Due to the sidecar thing we must use the messaging container as host
      NGINX_UPSTREAM_PORT: "8003"
      NGINX_CERT_NAME: "rasenmaeher"
    networks:
      - taknet
      - intranet
    ports:
      - "${TAK_RMAPI_PORT:-4626}:${TAK_RMAPI_PORT:-4626}"
    depends_on:
      miniwerk:
        condition: service_completed_successfully
      takrmapi:
        condition: service_healthy
    healthcheck:
      test: 'true'  # FIXME
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped
############
# End: TAK #
############


networks:
  productnet:
  apinet:
  kcnet:
  canet:
  ocspnet:
  dbnet:
  intranet:
  taknet:

volumes:
  kraftwerk_data:
  kwinit_data:
  kraftwerk_shared_fake:
  kraftwerk_shared_tak:
  kraftwerk_shared_rasenmaeher:
  pg_data:
  cfssl_data:
  openldap_data:
  rmapi_data:
  rmfpapi_data:
  ca_public:
  le_certs:
  tak_data:
  takrmapi_data:
  rmui_files:
